{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flickr data Mining script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zhongrui Ning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. import your flickr api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Flickr API\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from time import sleep\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "api_key ='Insert your key'\n",
    "api_secret = 'Insert your secret'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get the user data in the U.S. from 2015 to 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurred: 'photos'\n",
      "Error occurred: 'photos'\n",
      "Error occurred: 'photos'\n",
      "Error occurred: 'photos'\n",
      "Error occurred: 'photos'\n",
      "Error occurred: 'photos'\n",
      "Error occurred: 'photos'\n",
      "Error occurred: 'photos'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 142\u001b[0m\n\u001b[1;32m    139\u001b[0m             sleep(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 142\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 139\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError occurred: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 139\u001b[0m \u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from time import sleep\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Define the parameters\n",
    "EXTRAS = \"url_o,license,geo,lat,lon,tags,description,date_taken\"\n",
    "PER_PAGE = 250\n",
    "BASE_URL = \"https://www.flickr.com/services/rest/\"\n",
    "TEMP_FILE = 'flickr_fetch_temp.json'\n",
    "FINAL_FILE = 'flickr_images_sorted.csv'\n",
    "\n",
    "def daterange(start_date, end_date, delta_days=10):\n",
    "    while start_date < end_date:\n",
    "        yield start_date\n",
    "        start_date += timedelta(days=delta_days)\n",
    "\n",
    "def fetch_photos(min_date, max_date, retries=5):\n",
    "    params = {\n",
    "        \"method\": \"flickr.photos.search\",\n",
    "        \"api_key\": api_key,\n",
    "        \"min_taken_date\": min_date,\n",
    "        \"max_taken_date\": max_date,\n",
    "        \"extras\": EXTRAS,\n",
    "        \"format\": \"json\",\n",
    "        \"nojsoncallback\": \"1\",\n",
    "        \"per_page\": PER_PAGE,\n",
    "        \"page\": 1,  # Start at page 1\n",
    "        \"bbox\": \"-125.001650,24.9493,-66.9326,49.5904\"  # Coordinates for the US\n",
    "    }\n",
    "    all_photos = []\n",
    "    while True:\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                response = requests.get(BASE_URL, params=params)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    photos = data['photos']['photo']\n",
    "                    all_photos.extend(photos)\n",
    "                    \n",
    "                    # Get the total number of pages and stop if we've reached the last page\n",
    "                    total_pages = data['photos']['pages']\n",
    "                    current_page = params['page']\n",
    "                    \n",
    "                    if current_page >= total_pages:\n",
    "                        return all_photos\n",
    "                    \n",
    "                    params['page'] += 1  # Move to the next page\n",
    "                    break\n",
    "                else:\n",
    "                    print(f\"Error: Received status code {response.status_code}. Retrying...\")\n",
    "                    sleep(2 ** attempt)\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Network error: {e}. Retrying...\")\n",
    "                sleep(2 ** attempt)\n",
    "        else:\n",
    "            break\n",
    "    return all_photos\n",
    "\n",
    "def load_checkpoint():\n",
    "    if os.path.exists(TEMP_FILE):\n",
    "        try:\n",
    "            with open(TEMP_FILE, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                return data['date_ranges'], data['photos']\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Warning: Temp file {TEMP_FILE} is corrupted. Starting fresh. Error: {e}\")\n",
    "            # 如果文件损坏，删除它并返回空结果\n",
    "            os.remove(TEMP_FILE)\n",
    "            return [], []\n",
    "    return [], []\n",
    "\n",
    "def save_checkpoint(date_ranges, photos, all_photos):\n",
    "    # Combine existing photos with new photos and remove duplicates based on 'id'\n",
    "    combined_photos = all_photos + photos\n",
    "    unique_photos = {photo['id']: photo for photo in combined_photos}.values()\n",
    "\n",
    "    temp_file = TEMP_FILE + '.tmp'\n",
    "    # Save unique photos to a temporary file\n",
    "    with open(temp_file, 'w') as f:\n",
    "        json.dump({'date_ranges': date_ranges, 'photos': list(unique_photos)}, f)\n",
    "    \n",
    "    # After successful write, replace the original file\n",
    "    shutil.move(temp_file, TEMP_FILE)\n",
    "\n",
    "def save_to_csv(photos):\n",
    "    if os.path.exists(FINAL_FILE):\n",
    "        existing_df = pd.read_csv(FINAL_FILE)\n",
    "    else:\n",
    "        existing_df = pd.DataFrame()\n",
    "\n",
    "    new_df = pd.DataFrame(photos)\n",
    "    combined_df = pd.concat([existing_df, new_df]).drop_duplicates(subset='id')\n",
    "\n",
    "    combined_df.to_csv(FINAL_FILE, index=False)\n",
    "\n",
    "def process_date_range(min_date_str, max_date_str):\n",
    "    photos = fetch_photos(min_date_str, max_date_str)\n",
    "    return photos\n",
    "\n",
    "def main():\n",
    "    start_date = datetime.strptime(\"2015-01-01\", \"%Y-%m-%d\")\n",
    "    end_date = datetime.strptime(\"2023-12-31\", \"%Y-%m-%d\")\n",
    "\n",
    "# load checkpoint\n",
    "    completed_date_ranges, all_photos = load_checkpoint()\n",
    "\n",
    "    date_ranges = []\n",
    "    for single_date in daterange(start_date, end_date, delta_days=10):\n",
    "        min_date_str = single_date.strftime(\"%Y-%m-%d\")\n",
    "        max_date_str = (single_date + timedelta(days=10)).strftime(\"%Y-%m-%d\")\n",
    "        if [min_date_str, max_date_str] not in completed_date_ranges:\n",
    "            date_ranges.append((min_date_str, max_date_str))\n",
    "\n",
    "    if not date_ranges:\n",
    "        print(\"All date ranges have been processed.\")\n",
    "        return\n",
    "\n",
    "    # using multiple threads to process the date ranges\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        futures = {executor.submit(process_date_range, dr[0], dr[1]): dr for dr in date_ranges}\n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                photos = future.result()\n",
    "                if photos:\n",
    "                    all_photos.extend(photos)\n",
    "                    completed_date_ranges.append(futures[future])\n",
    "                    print(f\"Total photos fetched so far: {len(all_photos)}\")\n",
    "\n",
    "                    save_checkpoint(completed_date_ranges, photos, all_photos)\n",
    "                    save_to_csv(photos)\n",
    "            except Exception as e:\n",
    "                print(f\"Error occurred: {e}\")\n",
    "            sleep(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from time import sleep\n",
    "from sklearn.cluster import DBSCAN\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Flickr API key\n",
    "API_KEY = api_key\n",
    "EXTRAS = \"url_o,license,geo,lat,lon,tags,description,date_taken\"\n",
    "BASE_URL = 'https://www.flickr.com/services/rest/'\n",
    "PROGRESS_FILE = 'flickr_progress.json'\n",
    "OUTPUT_FILE = 'flickr_data_daily.csv'\n",
    "MAX_PAGES = 100 \n",
    "MAX_RETRIES = 5 \n",
    "\n",
    "# Fetch photos within a date range\n",
    "def fetch_photos(start_date, end_date, bbox=\"-125.001650,24.9493,-66.9326,49.5904\"):\n",
    "    all_photos = []\n",
    "    page = 1\n",
    "    retries = 0\n",
    "\n",
    "    while page <= MAX_PAGES:\n",
    "        params = {\n",
    "            \"method\": \"flickr.photos.search\",\n",
    "            \"api_key\": API_KEY,\n",
    "            \"min_taken_date\": start_date,\n",
    "            \"max_taken_date\": end_date,\n",
    "            \"bbox\": bbox,\n",
    "            \"extras\": EXTRAS,\n",
    "            \"format\": \"json\",\n",
    "            \"nojsoncallback\": \"1\",\n",
    "            \"per_page\": 250,\n",
    "            \"page\": page\n",
    "        }\n",
    "        response = requests.get(BASE_URL, params=params)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            retries += 1\n",
    "            if retries >= MAX_RETRIES:\n",
    "                print(f\"Failed to retrieve data after {MAX_RETRIES} retries for {start_date} to {end_date}.\")\n",
    "                break\n",
    "            print(f\"Retrying ({retries}/{MAX_RETRIES}) due to network issues...\")\n",
    "            sleep(2)\n",
    "            continue\n",
    "\n",
    "        data = response.json()\n",
    "        \n",
    "        if 'photos' not in data or 'photo' not in data['photos']:\n",
    "            print(f\"No data returned for {start_date} to {end_date}. Skipping this range.\")\n",
    "            break\n",
    "\n",
    "        photos = data['photos']['photo']\n",
    "        all_photos.extend(photos)\n",
    "        print(f\"Downloaded {len(photos)} photos on page {page} for date range {start_date} to {end_date}\")\n",
    "\n",
    "        if len(photos) < 250 or page >= data['photos']['pages']:\n",
    "            break\n",
    "        \n",
    "        page += 1\n",
    "    return all_photos\n",
    "\n",
    "# Daily date range generator for finer-grained data fetching\n",
    "def generate_date_ranges(start, end, delta_days=1):\n",
    "    current = start\n",
    "    while current < end:\n",
    "        yield current, current + timedelta(days=delta_days)\n",
    "        current += timedelta(days=delta_days)\n",
    "\n",
    "# Load previous progress if it exists\n",
    "def load_progress():\n",
    "    if os.path.exists(PROGRESS_FILE):\n",
    "        with open(PROGRESS_FILE, 'r') as f:\n",
    "            return json.load(f)\n",
    "    return {\"completed_ranges\": []}\n",
    "\n",
    "# Save progress to a JSON file\n",
    "def save_progress(completed_ranges):\n",
    "    with open(PROGRESS_FILE, 'w') as f:\n",
    "        json.dump({\"completed_ranges\": completed_ranges}, f)\n",
    "\n",
    "# Download and save data for each daily date range with progress tracking\n",
    "def collect_flickr_data(start_year, end_year):\n",
    "    progress = load_progress()\n",
    "    completed_ranges = progress[\"completed_ranges\"]\n",
    "\n",
    "    # Generate daily ranges and filter out already completed ranges\n",
    "    date_ranges = [(start_date, end_date) for start_date, end_date in generate_date_ranges(datetime(start_year, 1, 1), datetime(end_year, 12, 31))]\n",
    "    date_ranges = [dr for dr in date_ranges if dr[0].strftime(\"%Y-%m-%d\") not in completed_ranges]\n",
    "    \n",
    "    for start_date, end_date in date_ranges:\n",
    "        photos = fetch_photos(start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d'))\n",
    "        \n",
    "        if photos:\n",
    "            pd.DataFrame(photos).to_csv(OUTPUT_FILE, mode='a', header=not os.path.exists(OUTPUT_FILE), index=False)\n",
    "\n",
    "        # Update progress tracking\n",
    "        completed_ranges.append(start_date.strftime(\"%Y-%m-%d\"))\n",
    "        save_progress(completed_ranges)\n",
    "        \n",
    "        print(f\"Progress: {len(completed_ranges)}/{len(date_ranges)} days completed. Total photos collected so far: {len(photos)}\")\n",
    "        sleep(1)\n",
    "\n",
    "    print(\"Data collection complete.\")\n",
    "\n",
    "# Data Cleaning\n",
    "def clean_data(df):\n",
    "    df['latitude'] = pd.to_numeric(df['latitude'], errors='coerce')\n",
    "    df['longitude'] = pd.to_numeric(df['longitude'], errors='coerce')\n",
    "    df.dropna(subset=['latitude', 'longitude'], inplace=True)\n",
    "    df.drop_duplicates(subset='id', inplace=True)\n",
    "    return df\n",
    "\n",
    "# Temporal Analysis\n",
    "def temporal_analysis(df):\n",
    "    df['datetaken'] = pd.to_datetime(df['datetaken'], errors='coerce')\n",
    "    df['month'] = df['datetaken'].dt.to_period('M')\n",
    "    df.groupby('month').size().plot(kind='line', title='Temporal Distribution')\n",
    "    plt.show()\n",
    "\n",
    "# Spatial Clustering\n",
    "def spatial_clustering(df):\n",
    "    coords = df[['latitude', 'longitude']].values\n",
    "    clustering = DBSCAN(eps=0.05, min_samples=10, metric='haversine').fit(coords)\n",
    "    df['cluster'] = clustering.labels_\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.longitude, df.latitude))\n",
    "    gdf.plot(column='cluster', cmap='viridis', legend=True)\n",
    "    plt.show()\n",
    "\n",
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "    collect_flickr_data(2016, 2023)\n",
    "    \n",
    "    if os.path.exists(OUTPUT_FILE):\n",
    "        df = pd.read_csv(OUTPUT_FILE)\n",
    "        df = clean_data(df)\n",
    "        temporal_analysis(df)\n",
    "        spatial_clustering(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
