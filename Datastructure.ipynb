{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flickr data Mining script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zhongrui Ning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. import your flickr api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Flickr API\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from time import sleep\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "api_key ='f33d5e60551c0b35cfc78eb4aeee186c'\n",
    "api_secret = '1e96a5669c9157aa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = 'c2d834ecbe14197d2d21bcb0e473a2e1'\n",
    "api_secret = 'cb9eebb9aac6ac51'\n",
    "# back up key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = 'e7e7d9aa60b883213ae27da208f7ee4a'\n",
    "api_secret = '9807436dcdccbe3a'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get the user data in the U.S. from 2015 to 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All date ranges have been processed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from time import sleep\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Define the parameters\n",
    "EXTRAS = \"url_o,license,geo,lat,lon,tags,description,date_taken\"\n",
    "PER_PAGE = 250\n",
    "BASE_URL = \"https://www.flickr.com/services/rest/\"\n",
    "TEMP_FILE = 'flickr_fetch_temp.json'\n",
    "FINAL_FILE = 'flickr_images_sorted.csv'\n",
    "\n",
    "def daterange(start_date, end_date, delta_days=10):\n",
    "    while start_date < end_date:\n",
    "        yield start_date\n",
    "        start_date += timedelta(days=delta_days)\n",
    "\n",
    "def fetch_photos(min_date, max_date, retries=5):\n",
    "    params = {\n",
    "        \"method\": \"flickr.photos.search\",\n",
    "        \"api_key\": api_key,\n",
    "        \"min_taken_date\": min_date,\n",
    "        \"max_taken_date\": max_date,\n",
    "        \"extras\": EXTRAS,\n",
    "        \"format\": \"json\",\n",
    "        \"nojsoncallback\": \"1\",\n",
    "        \"per_page\": PER_PAGE,\n",
    "        \"page\": 1,  # Start at page 1\n",
    "        \"bbox\": \"-125.001650,24.9493,-66.9326,49.5904\"  # Coordinates for the US\n",
    "    }\n",
    "    all_photos = []\n",
    "    while True:\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                response = requests.get(BASE_URL, params=params)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    photos = data['photos']['photo']\n",
    "                    all_photos.extend(photos)\n",
    "                    \n",
    "                    # Get the total number of pages and stop if we've reached the last page\n",
    "                    total_pages = data['photos']['pages']\n",
    "                    current_page = params['page']\n",
    "                    \n",
    "                    if current_page >= total_pages:\n",
    "                        return all_photos\n",
    "                    \n",
    "                    params['page'] += 1  # Move to the next page\n",
    "                    break\n",
    "                else:\n",
    "                    print(f\"Error: Received status code {response.status_code}. Retrying...\")\n",
    "                    sleep(2 ** attempt)\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Network error: {e}. Retrying...\")\n",
    "                sleep(2 ** attempt)\n",
    "        else:\n",
    "            break\n",
    "    return all_photos\n",
    "\n",
    "def load_checkpoint():\n",
    "    if os.path.exists(TEMP_FILE):\n",
    "        try:\n",
    "            with open(TEMP_FILE, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                return data['date_ranges'], data['photos']\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Warning: Temp file {TEMP_FILE} is corrupted. Starting fresh. Error: {e}\")\n",
    "            # 如果文件损坏，删除它并返回空结果\n",
    "            os.remove(TEMP_FILE)\n",
    "            return [], []\n",
    "    return [], []\n",
    "\n",
    "def save_checkpoint(date_ranges, photos, all_photos):\n",
    "    # Combine existing photos with new photos and remove duplicates based on 'id'\n",
    "    combined_photos = all_photos + photos\n",
    "    unique_photos = {photo['id']: photo for photo in combined_photos}.values()\n",
    "\n",
    "    temp_file = TEMP_FILE + '.tmp'\n",
    "    # Save unique photos to a temporary file\n",
    "    with open(temp_file, 'w') as f:\n",
    "        json.dump({'date_ranges': date_ranges, 'photos': list(unique_photos)}, f)\n",
    "    \n",
    "    # After successful write, replace the original file\n",
    "    shutil.move(temp_file, TEMP_FILE)\n",
    "\n",
    "def save_to_csv(photos):\n",
    "    if os.path.exists(FINAL_FILE):\n",
    "        existing_df = pd.read_csv(FINAL_FILE)\n",
    "    else:\n",
    "        existing_df = pd.DataFrame()\n",
    "\n",
    "    new_df = pd.DataFrame(photos)\n",
    "    combined_df = pd.concat([existing_df, new_df]).drop_duplicates(subset='id')\n",
    "\n",
    "    combined_df.to_csv(FINAL_FILE, index=False)\n",
    "\n",
    "def process_date_range(min_date_str, max_date_str):\n",
    "    photos = fetch_photos(min_date_str, max_date_str)\n",
    "    return photos\n",
    "\n",
    "def main():\n",
    "    start_date = datetime.strptime(\"2015-01-01\", \"%Y-%m-%d\")\n",
    "    end_date = datetime.strptime(\"2023-12-31\", \"%Y-%m-%d\")\n",
    "\n",
    "# load checkpoint\n",
    "    completed_date_ranges, all_photos = load_checkpoint()\n",
    "\n",
    "    date_ranges = []\n",
    "    for single_date in daterange(start_date, end_date, delta_days=10):\n",
    "        min_date_str = single_date.strftime(\"%Y-%m-%d\")\n",
    "        max_date_str = (single_date + timedelta(days=10)).strftime(\"%Y-%m-%d\")\n",
    "        if [min_date_str, max_date_str] not in completed_date_ranges:\n",
    "            date_ranges.append((min_date_str, max_date_str))\n",
    "\n",
    "    if not date_ranges:\n",
    "        print(\"All date ranges have been processed.\")\n",
    "        return\n",
    "\n",
    "    # using multiple threads to process the date ranges\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        futures = {executor.submit(process_date_range, dr[0], dr[1]): dr for dr in date_ranges}\n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                photos = future.result()\n",
    "                if photos:\n",
    "                    all_photos.extend(photos)\n",
    "                    completed_date_ranges.append(futures[future])\n",
    "                    print(f\"Total photos fetched so far: {len(all_photos)}\")\n",
    "\n",
    "                    save_checkpoint(completed_date_ranges, photos, all_photos)\n",
    "                    save_to_csv(photos)\n",
    "            except Exception as e:\n",
    "                print(f\"Error occurred: {e}\")\n",
    "            sleep(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting the climate data and calculating anomalies and extreme weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
